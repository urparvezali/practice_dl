{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "train_data = datasets.FashionMNIST(\n",
    "\troot=\"data\",\n",
    "\ttrain=True,\n",
    "\tdownload=True,\n",
    "\ttransform=transforms.ToTensor(),\n",
    ")\n",
    "test_data = datasets.FashionMNIST(\n",
    "\troot=\"data\",\n",
    "\ttrain=False,\n",
    "\tdownload=True,\n",
    "\ttransform=transforms.ToTensor(),\n",
    ")\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from torch import randint\n",
    "\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "\n",
    "for i in range(1, 26):\n",
    "\trandom_img_no = randint(0, len(train_data),size=(1,)).item()\n",
    "\timg, lbl = train_data[random_img_no]\n",
    "\timg = img.squeeze()\n",
    "\tfig.add_subplot(5, 5, i)\n",
    "\tplt.imshow(img)\n",
    "\tplt.title(lbl)\n",
    "\tplt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "\tdataset=train_data,\n",
    "\tbatch_size=batch_size,\n",
    "\tshuffle=True,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "\tdataset=train_data,\n",
    "\tbatch_size=batch_size,\n",
    "\tshuffle=False,\n",
    ")\n",
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class FashionCNN(nn.Module):\n",
    "\tdef __init__(self, in_channels=3, hidden_units=9, out_channels=10, batch_size=32):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.batch_size = batch_size\n",
    "\n",
    "\t\tself.input_stack = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(\n",
    "\t\t\t\tin_channels=in_channels,\n",
    "\t\t\t\tout_channels=hidden_units,\n",
    "\t\t\t\tkernel_size=3,\n",
    "\t\t\t\tstride=1,\n",
    "\t\t\t\tpadding=1,\n",
    "\t\t\t),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Conv2d(\n",
    "\t\t\t\tin_channels=hidden_units,\n",
    "\t\t\t\tout_channels=hidden_units,\n",
    "\t\t\t\tkernel_size=3,\n",
    "\t\t\t\tstride=1,\n",
    "\t\t\t\tpadding=1,\n",
    "\t\t\t),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool2d(\n",
    "\t\t\t\tkernel_size=2,\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\t\tself.hidden_stack = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(\n",
    "\t\t\t\tin_channels=hidden_units,\n",
    "\t\t\t\tout_channels=hidden_units,\n",
    "\t\t\t\tkernel_size=2,\n",
    "\t\t\t\tstride=1,\n",
    "\t\t\t\tpadding=0,\n",
    "\t\t\t),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Conv2d(\n",
    "\t\t\t\tin_channels=hidden_units,\n",
    "\t\t\t\tout_channels=hidden_units,\n",
    "\t\t\t\tkernel_size=2,\n",
    "\t\t\t\tstride=1,\n",
    "\t\t\t\tpadding=0,\n",
    "\t\t\t),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool2d(\n",
    "\t\t\t\tkernel_size=2,\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\t\tself.output_stack = nn.Sequential(\n",
    "\t\t\tnn.Flatten(),\n",
    "\t\t\tnn.Linear(\n",
    "\t\t\t\tin_features=hidden_units*6*6,\n",
    "\t\t\t\tout_features=out_channels,\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\t\tx = self.input_stack(x)\n",
    "\t\t# print(x.shape)\n",
    "\t\tx = self.hidden_stack(x)\n",
    "\t\t# print(x.shape)\n",
    "\t\treturn self.output_stack(x)\n",
    "\n",
    "\tdef accuray(self, preds: torch.Tensor, real: torch.Tensor) -> float:\n",
    "\t\treturn torch.eq(preds.argmax(dim=1), real).sum().item()/self.batch_size\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mdl = FashionCNN(\n",
    "\thidden_units=8,\n",
    "\tin_channels=1,\n",
    "\tout_channels=10,\n",
    "\tbatch_size=batch_size,\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "\tparams=mdl.parameters(),\n",
    "\tlr=0.01,\n",
    ")\n",
    "mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "\tfor batch, (x, y) in enumerate(train_loader):\n",
    "\t\tmdl.train()\n",
    "\t\tpreds = mdl(x)\n",
    "\n",
    "\t\tloss = loss_fn(preds, y)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\tmdl.eval()\n",
    "\twith torch.inference_mode():\n",
    "\t\tacc, loss = 0, 0\n",
    "\t\tfor test_x, test_y in test_loader:\n",
    "\t\t\ttest_preds = mdl(test_y)\n",
    "\t\t\tloss += loss_fn(test_preds, test_y)\n",
    "\t\t\tacc += mdl.accuray(test_preds, test_y)\n",
    "\t\tacc/=len(test_loader)\n",
    "\t\tloss/=len(test_loader)\n",
    "\n",
    "\t\tprint(f\"epoch:{epoch:<4} acc:{acc:<6} loss:{loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
